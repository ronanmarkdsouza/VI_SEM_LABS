{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Artificial Intelligence Lab-2**\n",
        "### Ronan Mark D'souza\n",
        "### 200968256\n",
        "### *Data Science & Engg.*\n",
        "#### *Manipal Institute of Technology, Manipal*"
      ],
      "metadata": {
        "id": "qcOx4T-hEddg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing Libraries:"
      ],
      "metadata": {
        "id": "7FUDnLF_EizI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tf-agents[reverb]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8ujJTW1MEd0",
        "outputId": "9264e15c-65e6-48e4-ff2a-aecaa746f1d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m624.4/624.4 KB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.4/48.4 KB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m92.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-i1jVLpEZUM"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import abc\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from tf_agents.environments import py_environment\n",
        "from tf_agents.environments import tf_environment\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.environments import utils\n",
        "from tf_agents.specs import array_spec\n",
        "from tf_agents.environments import wrappers\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.trajectories import time_step as ts"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CartPole-v0:"
      ],
      "metadata": {
        "id": "3JfKj5atN43c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Implement the CartPole environment for a certain number of steps:"
      ],
      "metadata": {
        "id": "9OwI1lN3OCW-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = suite_gym.load('CartPole-v0')\n",
        "\n",
        "tf_env = tf_py_environment.TFPyEnvironment(env)\n",
        "# reset() creates the initial time_step after resetting the environment.\n",
        "time_step = tf_env.reset()\n",
        "num_steps = 3\n",
        "transitions = []\n",
        "reward = 0\n",
        "for i in range(num_steps):\n",
        "  action = tf.constant([i % 2])\n",
        "  # applies the action and returns the new TimeStep.\n",
        "  next_time_step = tf_env.step(action)\n",
        "  transitions.append([time_step, action, next_time_step])\n",
        "  reward += next_time_step.reward\n",
        "  time_step = next_time_step\n",
        "\n",
        "np_transitions = tf.nest.map_structure(lambda x: x.numpy(), transitions)\n",
        "print('\\n'.join(map(str, np_transitions)))\n",
        "print('Total reward:', reward.numpy())"
      ],
      "metadata": {
        "id": "RhZVGaWRLam1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ac046da-a808-4376-9608-edf1396afcd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TimeStep(\n",
            "{'discount': array([1.], dtype=float32),\n",
            " 'observation': array([[ 0.01973564, -0.01295051, -0.04152805,  0.03892811]],\n",
            "      dtype=float32),\n",
            " 'reward': array([0.], dtype=float32),\n",
            " 'step_type': array([0], dtype=int32)}), array([0], dtype=int32), TimeStep(\n",
            "{'discount': array([1.], dtype=float32),\n",
            " 'observation': array([[ 0.01947663, -0.2074531 , -0.04074949,  0.31822473]],\n",
            "      dtype=float32),\n",
            " 'reward': array([1.], dtype=float32),\n",
            " 'step_type': array([1], dtype=int32)})]\n",
            "[TimeStep(\n",
            "{'discount': array([1.], dtype=float32),\n",
            " 'observation': array([[ 0.01947663, -0.2074531 , -0.04074949,  0.31822473]],\n",
            "      dtype=float32),\n",
            " 'reward': array([1.], dtype=float32),\n",
            " 'step_type': array([1], dtype=int32)}), array([1], dtype=int32), TimeStep(\n",
            "{'discount': array([1.], dtype=float32),\n",
            " 'observation': array([[ 0.01532757, -0.01177518, -0.03438499,  0.01297446]],\n",
            "      dtype=float32),\n",
            " 'reward': array([1.], dtype=float32),\n",
            " 'step_type': array([1], dtype=int32)})]\n",
            "[TimeStep(\n",
            "{'discount': array([1.], dtype=float32),\n",
            " 'observation': array([[ 0.01532757, -0.01177518, -0.03438499,  0.01297446]],\n",
            "      dtype=float32),\n",
            " 'reward': array([1.], dtype=float32),\n",
            " 'step_type': array([1], dtype=int32)}), array([0], dtype=int32), TimeStep(\n",
            "{'discount': array([1.], dtype=float32),\n",
            " 'observation': array([[ 0.01509206, -0.20638756, -0.0341255 ,  0.29461327]],\n",
            "      dtype=float32),\n",
            " 'reward': array([1.], dtype=float32),\n",
            " 'step_type': array([1], dtype=int32)})]\n",
            "Total reward: [3.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Implement the CartPole environment for a certain number of episodes:"
      ],
      "metadata": {
        "id": "6Eh-b3bNOMks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = suite_gym.load('CartPole-v0')\n",
        "tf_env = tf_py_environment.TFPyEnvironment(env)\n",
        "\n",
        "time_step = tf_env.reset()\n",
        "rewards = []\n",
        "steps = []\n",
        "num_episodes = 5\n",
        "\n",
        "for _ in range(num_episodes):\n",
        "  episode_reward = 0\n",
        "  episode_steps = 0\n",
        "  while not time_step.is_last():\n",
        "    action = tf.random.uniform([1], 0, 2, dtype=tf.int32)\n",
        "    time_step = tf_env.step(action)\n",
        "    episode_steps += 1\n",
        "    episode_reward += time_step.reward.numpy()\n",
        "  rewards.append(episode_reward)\n",
        "  steps.append(episode_steps)\n",
        "  time_step = tf_env.reset()\n",
        "\n",
        "num_steps = np.sum(steps)\n",
        "avg_length = np.mean(steps)\n",
        "avg_reward = np.mean(rewards)\n",
        "\n",
        "print('num_episodes:', num_episodes, 'num_steps:', num_steps)\n",
        "print('avg_length', avg_length, 'avg_reward:', avg_reward)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ozhoIrYOQhw",
        "outputId": "fe785099-3c2c-4824-f131-427b9b509a70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_episodes: 5 num_steps: 69\n",
            "avg_length 13.8 avg_reward: 13.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above results show us that reward is higher in episodes when compared to steps."
      ],
      "metadata": {
        "id": "fc8C1ix34bYX"
      }
    }
  ]
}